{
    "collab_server" : "",
    "contents" : "############################################################# \n## Stat 202A - Homework 1\n## Author: Hariharan Shanmugavadivel \n## Date : 10/10/2017\n## Description: This script implements linear regression \n## using Gauss-Jordan elimination in both plain and\n## vectorized forms\n#############################################################\n\n#############################################################\n## INSTRUCTIONS: Please fill in the missing lines of code\n## only where specified. Do not change function names, \n## function inputs or outputs. You can add examples at the\n## end of the script (in the \"Optional examples\" section) to \n## double-check your work, but MAKE SURE TO COMMENT OUT ALL \n## OF YOUR EXAMPLES BEFORE SUBMITTING.\n##\n## Very important: Do not use the function \"setwd\" anywhere\n## in your code. If you do, I will be unable to grade your \n## work since R will attempt to change my working directory\n## to one that does not exist.\n##\n## Do not use the following functions for this assignment,\n## except when debugging or in the optional examples section:\n## 1) lm()\n## 2) solve()\n#############################################################\n\n\n###############################################\n## Function 1: Plain version of Gauss Jordan ##\n###############################################\n\n\nmyGaussJordan <- function(A, m){\n  \n  # Perform Gauss Jordan elimination on A.\n  # \n  # A: a square matrix.\n  # m: the pivot element is A[m, m].\n  # Returns a matrix with the identity matrix \n  # on the left and the inverse of A on the right. \n\n  #############################################\n  ## FILL IN THE BODY OF THIS FUNCTION BELOW ##\n  #############################################\n  \n  n <- dim(A)[1]\n  B <- cbind(A, diag(rep(1,n)))\n\n  \n  for(k in 1:m)\n  {\n    a <- B[k,k]\n    \n    for(j in 1:(n*2))\n    {\n      B[k,j] <- B[k,j]/a\n    }\n    \n    for(i in 1:n)\n    {\n      if(i != k)\n      {\n        b <- B[i,k]\n        \n        for(j in 1:(n*2))\n          \n          B[i,j] <- B[i,j] - b*B[k,j]\n        \n      }\n    }\n  }\n  \n  ## Function returns the matrix B\n\n  return(B)\n  \n}\n\n####################################################\n## Function 2: Vectorized version of Gauss Jordan ##\n####################################################\n\nmyGaussJordanVec <- function(A, m){\n  \n  # Perform Gauss Jordan elimination on A.\n  # \n  # A: a square matrix.\n  # m: the pivot element is A[m, m].\n  # Returns a matrix with the identity matrix \n  # on the left and the inverse of A on the right.\n  \n  #############################################\n  ## FILL IN THE BODY OF THIS FUNCTION BELOW ##\n  #############################################\n  \n  n <- dim(A)[1]\n  B <- cbind(A, diag(rep(1,n)))\n  \n  for(k in 1:m)\n  {\n    B[k, ] <- B[k, ] / B[k,k]\n    \n    for(i in 1:n)\n      if(i != k)\n        B[i,] <- B[i,] - B[k, ] * B[i,k]\n  }\n\n  \n  ## Function returns the matrix B\n  return(B)\n  \n}\n\n\n\n######################################################\n## Function 3: Linear regression using Gauss Jordan ##\n######################################################\n\nmyLinearRegression <- function(X, Y){\n  \n  # Find the regression coefficient estimates beta_hat\n  # corresponding to the model Y = X * beta + epsilon\n  # Your code must use one of the 2 Gauss Jordan \n  # functions you wrote above (either one is fine).\n  # Note: we do not know what beta is. We are only \n  # given a matrix X and a vector Y and we must come \n  # up with an estimate beta_hat.\n  # \n  # X: an 'n row' by 'p column' matrix of input variables.\n  # Y: an n-dimensional vector of responses\n\n  #############################################\n  ## FILL IN THE BODY OF THIS FUNCTION BELOW ##\n  #############################################\n  \n  ## Let me start things off for you...\n  n <- nrow(X)\n  p <- ncol(X)\n  \n  Z <- cbind(rep(1,n),X, Y)\n  A <- t(Z) %*% Z\n  S <- myGaussJordanVec(A, p+1)\n\n  \n  beta_hat <- S[1:(p+1),p+2]\n  \n  \n  ## Function returns the (p+1)-dimensional vector \n  ## beta_hat of regression coefficient estimates\n  print(beta_hat)\n  return(beta_hat)\n  \n}\n\n########################################################\n## Optional examples (comment out before submitting!) ##\n########################################################\n\n# testing_Linear_Regression <- function(){\n# \n#   ## This function is not graded; you can use it to\n#   ## test out the 'myLinearRegression' function\n# \n#   ## Define parameters\n#   n    <- 100\n#   p    <- 3\n# \n#   ## Simulate data from our assumed model.\n#   ## We can assume that the true intercept is 0\n#   X    <- matrix(rnorm(n * p), nrow = n)\n#   beta <- matrix(1:p, nrow = p)\n# \n#   Y    <- X %*% beta + rnorm(n)\n# \n# \n#   ## Save R's linear regression coefficients\n#   R_coef  <- coef(lm(Y ~ X))\n# \n# \n#   ## Save our linear regression coefficients\n#   my_coef <- myLinearRegression(X, Y)\n# \n#   ## Are these two vectors different?\n#   print(R_coef)\n#   print(my_coef)\n#   sum_square_diff <- sum((R_coef - my_coef)^2)\n#   if(sum_square_diff <= 0.001){\n#     return('Both results are identical')\n#   }else{\n#     return('There seems to be a problem...')\n#   }\n# \n# }\n\n",
    "created" : 1511935000973.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3131673608",
    "id" : "BC0C1168",
    "lastKnownWriteTime" : 1507863788,
    "last_content_update" : 1507863788,
    "path" : "D:/UCLA/Fall 2017/STATS202A - Stats Programming/Assignments/Assignment 1/Linear_Regression.r",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}